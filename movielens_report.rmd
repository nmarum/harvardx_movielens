---
title: "HarvardX Movielens Capstone Project"
author: "Nick Marum"
date: "01/09/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(caret)
library(lubridate)
library(recommenderlab)
library(recosystem)
#Analysis assumes edx and validation objects are saved individually in your working directory as edx.rda and validation.rda.  The edx and validation objects were created through separate import script as per the code provided by course.  

if(!exists("edx")) load("edx.rda") 
edx <- edx %>% mutate(datetime = as_datetime(timestamp)) #adding datetime column using timestamp

```

*INTRODUCTION*
(introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed)

This is a Movielens project report for the Harvardx Data Science professional program Capstone course. Inspired by the Netflix Prize (https://en.wikipedia.org/wiki/Netflix_Prize), this  machine learning project will use the 10 million entries from the "Movielens" data set, a publicly available data set user movie ratings, in order to predict movie user ratings based upon previous ratings the user  made of other movies.  Such an algorithm is the basis for movie recommendation systems used in popular movie streaming services.

Specifically, this report will demonstrate the steps taken to develop a machine learning algorithm to predict in a randomly selected hold out validation set made up of 10% of the 10 million entries.  The root mean squared error (RMSE) of the algorithm against the true values in the validation set will be used as part of the overall scoring of this project. (The validation set is not to be used for the training, evaluation or cross-validation of the algorithm.)  

_Overview of the dataset_

The project instructions require the use of the "edx" object as the training set portion of the project.  The original edx set included six columns: "userID", which provide a unique user identification number; "movieID", which provided a unique identification number for each move; "rating", the rating assigned by the user for the movie; and "timestamp", which was the time the user rating was recorded in POSIXct format.  Since "timestamp" is not easily interpretable, I added a seventh column, "datetime" which translated the timestamp into an accessible date and time format for each movie rating entry.  In all there are little more than 9 million movie rating entries from more than 70,000 users rating about 10,000 movies in the dataset  The earliest movie rating entry is from January 1995, and the latest is from January 2009.  Ratings were given in .5 increments between .5 to 5 stars (there is no 0 entries in the data set).  Ratings half star ratings were generally less common than full star ratings.  Four and three stars were the most common ratings, followed by 5 stars and 3.5 stars.



```{r cars}
summary(edx)
```

Ratings were given in .5 increments between .5 to 5 stars (there is no 0 entries in the data set).  Ratings half star ratings were generally less common than full star ratings.  Four and three stars were the most common ratings, followed by 5 stars and 3.5 stars.
```{r}
hist(edx$rating)
```


```{r}
qplot(movieId, data = edx, geom = "density")
```


*Methodology and Analysis*
The text book materials for the HarvardX machine learning course included a detailed discussion of the Movielens data and went son to examine a number of prediction methodologies, including taking into account user and movie effects as well as regularizing movie effects by minimizing the impact of outlier ratings on less commonly reviewed movies.  My intent is to start by recreating the most successful of the methodologies discussed in the text book (Regularized Movie + User Effect model:  .881 RMSE) and attempt to improve upon it.

The text book did provide a  clue as to how to improve on the model: using matrix factorization to identify latent patterns within the residuals of the Regularized Movie + User effect model (Chapter 33.11 - Matrix Factorization).  Using single value decomposition and principle component analysis, the course materials showed  there were  patterns left in the data in terms of groupings of movies that generally are rated similarly, which could be a basis for further improving on the model.  While the textbook did not discuss techniques that would use these estimates to fit a model, it recommended using the recommenderlab package for those students who were interested.

_recommenderlab_

The recommenderlab: Lab for Developing and Testing Recommender Algorithms R package version 0.2-6 (https://github.com/mhahsler/recommenderlab) is a framework for the testing and evaluation of recommendation algorithms.  The vignette for recommenderlab describes that there are various types of recommendation algorithms that are used in commercial applications, the type of algorithms that the package used are "collaborative filtering" algorithms which uses given ratings data from a variety of users on a set of items in order to make predictions of missing ratings - which seems to be an ideal algorithm for the Movielens project.   There are two types of collaborative filtering: user-based collaborative filtering and item-based collaborative filtering.  The package includes a number of helpful functions for recommendation tasks, including functions to normalize ratings data, transform ratings data into binary data, and a built-in RMSE function for estimating algorithm performance. _recommenderlab_ appears to be one of the most popular purpose built packages for recommendation systems using matrix factorization, however there are other packages as well.  For example, _recosystem_ uses a different computational approach for recommender system matrix factorization but does not have the same number of features and algorithms as the _recommenderlab_ package.  We will discuss _recosystem_ in more detail later.

User-based collaborative filtering (UBCF) assumes that users with similar will rate items similarly and therefore analyzes the ratings of many individuals in order to make predictions using a k-nearest neighbours approach.  The vignette also suggests that for some datasets, the overall performance of the algorithm can be improved if user rating data is normalized by subtracting the mean of each user row from each rating in pre-processing to remove user rating bias. (This approach is computationally intensive, which could be a challenge for me since I am using an old laptop.)

Item-based collaborative filtering (IBCF) is based upon the relationships between items inferred from the rating matrix.  The assumption behind this approach is that users will prefer items that are similar to items that they have already indicated they like.  This approach creates a similarity matrix for items which is used to make predictions.  This approach is less computationally intensive and therefore quicker than the UBCF, however it generally provides slightly inferior results.  Similar to UBCF, the package materials recommends normalizing the data in order to help improve results.

There is also other methods based upon single value decomposition (SVD) or matrix factorization which was described in the textbook as a potential methodology to improve upon model results as this was closely related to principle component analysis.

This project will examine collaborative filtering, single value decomposition and other matrix factorization algorithms to try and improve on the results of the text book Regularized Movie + User effect model as a baseline model.

_Planned Steps_

As a first step, we will break down the edx data into a small training and test set.  Then we will reconstruct a baseline model through the regularization of movie effects and the normalization of user effects as a pre-processing step.  We will then compare this baseline model with models developed within the _recommenderlab_ environment as a proof of concept.

If all goes well, we will split the larger edx dataset into training and test sets which will be imported into the recommender system environment in order to apply the highest performing algorithm on the larger data set and tune appropriately to provide the best result.

*Analysis*

As a first step we will create a small subset of the 100,000 entries from the edx dataset and create a training and a test set.
```{r}
set.seed(200907, sample.kind = "Rounding")#setting seed for replication later 
mini <- edx[1:100000] #first 100K entries

ind <- createDataPartition(y= mini$rating, times = 1, p = .2, list = FALSE)
train_mini <- mini[-ind,]
test_mini <- mini[ind,]

test_mini <- test_mini %>% 
  semi_join(train_mini, by = "movieId") %>%
  semi_join(train_mini, by = "userId")

summary(train_mini)
```
We will then normalize the user ratings to adjust for user rating bias by centering each rating by subtracting mean rating score (mu) for each user.

```{r}

mu <- train_mini %>% group_by(userId) %>%
  summarise(mu = mean(rating))
  
train_usernorm <- train_mini %>% left_join(mu, by="userId") %>%
  mutate(rating_norm = rating-mu)

train_usernorm %>% group_by(rating) %>% ggplot(aes(rating, rating_norm)) + geom_point(alpha = .5)

```

We will then regularize the movie data to adjust for movie effects.  First we will demonstrate the regularization of movie ratings with tuning parameter (lambda).  We will then show how we picked the ideal tuning parameter for the regularization.

```{r}

lambda <- .5
mu <- mean(train_mini$rating)
movie_reg_avgs <- train_mini %>% 
  group_by(movieId) %>% 
  summarize(regularized = sum(rating - mu)/(n()+lambda), n_i = n())

movie_avg <- train_mini %>%
  group_by(movieId) %>%
  summarise(avg = mean(rating))

plot(movie_avg$avg, movie_reg_avgs$regularized)
```
As we can see from the above plot, the regularization not only normalizes the data but also moderates some of the extreme ratings (1s and 5s) for films where there is relatively few ratings.  I should note that unlike the textbook which regularized both the user and movie rating data, I did not regularize the user data.  There was a good case made in the text book for regularizing the data for movie effects for obscure movies where there were relatively few ratings, normalizing the user effect (i.e, the tendency to rate movies in general more positively or negatively due to the personality of the reviewer) seemed to be sufficient.

The lambda parameter can be tuned to achieve the best results in terms of root mean squared error.  Below is the process for selecting the optimal lambda.  Note that we a selecting the optimized lambda based purely on the training data, but will use the test data to evaluate the overall performance.

```{r}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_mini$rating)
  
  b_i <- train_mini %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l)) #regularized film effect
  
  b_u <- train_mini %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu)) #normalized user effect

  predicted_ratings <- 
    train_mini %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
    return(RMSE(train_mini$rating, predicted_ratings)) #Note: using training set
})

lambdas[which.min(rmses)] #selecting the lambda with the best RMSE score 
plot(lambdas, rmses)
```

Using the training set data, we can see the optimized lambda is .5.  We will use this parameter in order to make predictions using the textbook based model on the test set.

```{r}

mu <- mean(train_mini$rating) #overall training wet average movie rating
l_opt <- .5 #lambda selected from training set optimization
  
  b_i <- train_mini %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l_opt)) #regularized movie effect
  
  b_u <- train_mini %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu)) # normalized user effect

  predicted_ratings <- 
    test_mini %>% #using test set for prediction
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>% #prediction based on overall set average, user and movie effects
    pull(pred)
  
RMSE(test_mini$rating, predicted_ratings)


```
Using the training set for optimizing the model and only using the test set for evaluation, can see that the overall performance is a little poorer than what was identified in the text book (.892 rather than .881).  However if the test data is used to optimize the parameter (which was what was done in the textbook), the results is quite similar to textbook regularized effects model as we can see below.

```{r}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_mini$rating)
  
  b_i <- train_mini %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_mini %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu))

  predicted_ratings <- 
    test_mini %>% # note we are now using the test set for the predictions
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
    return(RMSE(predicted_ratings, test_mini$rating))#RMSE based on test set
})

mu <- mean(train_mini$rating)
l <- lambdas[which.min(rmses)] #selecting the lambda with the best RMSE score
  
  b_i <- train_mini %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_mini %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu))

  predicted_ratings <- 
    test_mini %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
    RMSE(predicted_ratings, test_mini$rating)

```
Using the test data, we find that the optimal lambda parameter is 4.25.

Now that we have our baseline model, we will explore the algorithms available for us in the recommenderlab package.

*_recommenderlab_ Collaborative Filtering, SVD and other models*

The first step is to build a rating matrix that can be entered into the _recommenderlab_ "rating matrix" format.  One of the requirements of the _recommenderlab_ package is that each user is represented by a single row and that each item (or film in this case) is a column.  Given most users will only rate a handful of potential movies, this creates a "sparse" matrix comprised primarily of NAs that _recommenderlab_ stores is a less memory intensive way for computations.
```{r}

rm <- as(mini, "realRatingMatrix")

hist(getRatings(rm), breaks = 15)

```

We have imported our mini data set into the recommenderlab environment.  We will also take a look at using some of the built in functionality in recommenderlab to normalize the data, which is a recommended step before applying the recommender models built into the package.



```{r}
rm <- normalize(rm) #normalize function which normalizes data by row (i.e., user)

hist(getRatings(rm))
```



There are a number of different recommender algorithms in the _recommenderlab_ that can be accessed using the command:  recommenderRegistry$get_entries(dataType = "realRatingMatrix").  It includes single value decomposition (SVD) based methods in addition to the IBCF and UBCF methods.

We will try using an evaluation methodology (evaluationScheme) built into the package to train and evaluate a "recommender" model.


```{r}

e <- evaluationScheme(rm, method="split", train=.9, given=10, goodRating=5)
#building an evalutation scheme object from "realRatingMatrix".  90% of matrix will be used for training.  The given parameter is how many entries will be given to the model per user.  It is an adjustable parameter with a default of 10.  The "goodrating" parameter is simply to identify what a top rating would be under the model.

IBCF <- Recommender(getData(e, "train"), method="IBCF") 
#training recommender model, in this case, an IBCF, using the training set from the evaluation scheme object "e".
IBCF_preds <- predict(IBCF, getData(e, "known"), type="ratings")
#using the "known" user and item values from the test set in the evaluation scheme object to create a prediction of ratings (IBCF_preds)
IBCF_acc <- calcPredictionAccuracy(IBCF_preds, getData(e, "unknown"))
#accuracy is measured using the built in "calcPredictionAccuracy" function against the "unknown" part of the test set in the eval scheme object (i.e., ratings) and the model predictions.

UBCF <- Recommender(getData(e,"train"), method="UBCF")
UBCF_preds <- predict(UBCF, getData(e, "known"), type="ratings")
UBCF_acc <- calcPredictionAccuracy(UBCF_preds, getData(e, "unknown"))

SVDF <- Recommender(getData(e, "train"), method="SVDF")
SVDF_preds <- predict(SVDF, getData(e, "known"), type="ratings")
SVDF_acc <- calcPredictionAccuracy(SVDF_preds, getData(e, "unknown"))

SVD <- Recommender(getData(e, "train"), method="SVD")
SVD_preds <- predict(SVD, getData(e, "known"), type="ratings")
SVD_acc <- calcPredictionAccuracy(SVD_preds, getData(e, "unknown"))

data.frame(IBCF_acc, UBCF_acc, SVDF_acc, SVD_acc)
#building and printing a data frame with results of each method.
```

Much to my surprise, the collaborative filtering methods I thought would be so powerful did not do nearly as well as I would have hoped.  The Funk Single Value Decomposition (SVDF) appears to be the model that provides the best accuracy, however the accuracy is much inferior to the much simpler baseline model we developed.  The SVD approximation method also did well and was much less computationally intensive and generated results much faster.  It is possible that a larger dataset could improve the performance of the collaborative filtering and single value decomposition models, however tuning of the models could also improve results substantially.

Lets see if we can evaluate it in a different way using training and test sets and if it is possible to tune the performance.  



```{r}
reg_SVD <- recommenderRegistry$get_entry("SVD")
reg_SVDF <- recommenderRegistry$get_entry("SVDF")
data.frame(SVD = as.vector(reg_SVD$parameters))

```
Opening up and inspecting the registry entries for the SVD algorithm shows that there are essentially three parameters that are open to tuning.  "k" as in nearest neighbors in terms of features, which has a default value of 10, "maxiter" which has a default value of 100, and "normalize" which has a default value of "center".  Presumably the normalize can also be z-score normalized (or "scaled") which is another normalization technique described in the _recommenderlab_ documentation.


```{r}
data.frame(SVDF = as.vector(reg_SVDF$parameters))
```
However, SVDF has, in addition to the SVD parameters of "k" and "normalize", it also has "gamma" a regularization term, "lambda", the learning rate, "min_improvement", the requirement minimum improvement per iteration, "min_epochs", the minimum number of iterations per feature, "max_epochs", the maximum number of iterations per feature and "verbose" which is a logical parameter of showing progress.  Clearly SVDF is a more sophisticated and complicated algorithm to tune.

Lets try by tuning the SVD model by tuning for "k".
```{r}
e <- evaluationScheme(rm, method="cross", k=10, train=.9, given=10, goodRating=5)#changing up evaluation scheme to include k-fold cross-validation
ks <- seq(18, 38) #inputing a sequence of k-values for optimization.

rmses <- sapply(ks, function(ks){
results <- evaluate(e, method="SVD", type="ratings", param = list(k=ks))
avg(results)[1]
})

ks[which.min(rmses)]#selecting k value with min RMSE
qplot(ks, rmses) #plotting results

```

After a series of trial and error of different sequences of integer values for "k", I was finding a somewhat linear trend of better performance due to higher level of k values, with 28 being the maximum value - much more than the default of 10.  This raised raised concerns of overtraining for me.  As such, using the evaluation scheme of _recommenderlab_ I introduced k-fold cross-validation which suggested an optimal value of 37 which provided an RMSE of .9995.  A slight improvement.

Using this knowledge gained from the computationally less intensive SVD model, I would like to see how much we can improve the SVDF model by simply inputing the optimal the optimal SVD k-value of 37.

After an extremely long processing time and using a k parameter of 37, the resulting RMSE is .8986.  Still short of the .8819 of our baseline model but getting closer.  Additionally tuning could refine the result, but it would take a significant amount of time with relatively little improvement as we saw with the SVD model tuning.  This is still way short of the grading-scale full-points standard of .8649.

However, given what we learned through the textbook of the importance of not only normalizing data, but regularizing the data so as not to allow it to be skewed by few entries, and how that significantly improved our baseline model, I tried doing the same thing here.  

```{r}
mu <- mean(train_mini$rating)
movie_reg <- mini %>% 
  group_by(movieId) %>%
  summarize(rating = sum(rating-mu)/(n()+4.25)) 
#Regularization using lambda parameter identified earlier on the mini-test set.

movie_avg <- mini %>%
  group_by(movieId) %>%
  summarize(avg = mean(rating))

plot(movie_reg$rating~movie_avg$avg)

```

Above is showing the effect of the regularization using a larger lambda parameter (4.25) identified through tuning the baseline model against the test data set.  You can see a much more significant impact on the scores.


```{r}
mu <- mean(train_mini$rating)
mini_reg <- mini %>% 
  group_by(movieId) %>%
  mutate(rating = sum(rating-mu)/(n()+4.25)) %>% 
  ungroup()

hist(mini_reg$rating)
```

A histogram shows a tighter concentration of centre normalized scores around zero.


```{r}

rm_reg <- as(as.data.frame(mini_reg), "realRatingMatrix")
rm_reg <- normalize(rm_reg) #normalize entries by row (e.g., user)

hist(getRatings(rm_reg))

```

As expected, ratings are centered around zero.   A little more tight than just normalizing the data by user (row). 

If we examine the rating matrix as a data frame we can get see how the data is captured.

```{r}

head(as(rm_reg, "data.frame"))
```
We can see the top rating of the first 6 rows are all from the same user, user "1".  We see that item 292 is the top rating of the 6 films rated by this user while the lowest ratings are item 355 and item 122.  Let's inspect these items closer to see what we are dealing with.

```{r}

c(mini_reg$title[292], mini_reg$title[122], mini_reg$title[355])

```
The top rated film is the first entry in the data frame above, while the lowest rankings are the bottom two.

All three are romantic films, however Bridget Jone's Diary, the top rated film of the 6, was a romantic comedy blockbuster while the other two are more "arthouse" films.  "Three Colors: White" is a french language romantic drama and "The Sum of Us" is a Australian LGBT romantic comedy.  This all seems consistent with the film choices of a person who likes to watch romantic comedies however who prefers more popular Hollywood films over arthouse features.


Now We will take a look at a particular user to get a sense for the matrix.  In this case I have have chosen user "102".

```{r}
sub_set <- rm_reg[102]
as(sub_set, "matrix")[,1:50]


```
You can see that for user entry 102, the matrix is pretty sparse.  Only two entries in the first 50 columns (films).  This was actually better than most (and the reason why I picked 102) as generally speaking users did not rate any of the first 50 films.

The scores have been normallized so rather than seeing a raw rating of .5 to 5, we are seeing the adjusted scores.  Positive scores reflected higher than average rated movies and negative scores reflect lower than average scoring for that user.

```{r}
max <- which.max(as(sub_set, "matrix"))
min <- which.min(as(sub_set, "matrix"))

c(as(sub_set, "matrix")[,max], as(sub_set, "matrix")[,min])

```
We can see the range of scores above for user "102".  They are centered around 0 and it appears the user doesn't necessarily skew his scores one way or another.


Now that we have gotten a sense for the baseline model data (i.e., adjusted for movie and user effects), we will seek to apply the optimized k value using the SVD approximation model to see if we can get an improved score.

```{r}

e_reg <- evaluationScheme(rm_reg, method="cross", k=10, given=10, goodRating=5)
SVD_opt <- Recommender(getData(e_reg, "train"), method="SVD", para = list(k=37))
SVD_preds <- predict(SVD_opt, getData(e_reg, "known"), type="ratings")
calcPredictionAccuracy(SVD_preds, getData(e_reg, "unknown"))

```
Wow!  RMSE score of .429!  This was initially exciting but it also raised a red flag.  I realized in normalizing and regularizing the data I did so to the entire rating matrix, including to the values that make up the test set, before building my evaluation object.  Naturally modifying these values would have an effect on a the calculated RMSE.  The key to solving this appeared to be to try and apply this approach to objects other than the built in evaluation scheme that _recommenderlab_ uses, however that is where I ran into significant challenges.

While this SVD matrix factorization approach appeared very promising and seems to produce significant improvements to the baseline model , the _recommenderlab_ environment is somewhat self-contained, designed to use its own tools like the evaluation scheme for training and evaluating models.  It does not easily apply to the edx and validation models separately.  I tried to run the model developed in the recommenderlab environment on the code on the edx and a holdout dataset, I ran into memory errors (for example, an error regarding R's inability to handle a multi-gigabyte sized vector) and it simply would not run.  Very unfortunate and discouraging after having invested so much time in the package.

Before completely abandoning the matrix factorization approach, I sought out other packages that may be able to use a similar approach but are not in such a self contained environment that the parameters of this project would prevent me from using them.

Through monitoring the Movielens course bulletin boards I saw references to the _recosystem_ package.  It is similar to the _recommenderlab_ package in that it provides tools specifically for recommendation systems and leverages matrix factorization-based algorithms.  Also, it has some features that are specifically designed to limit the computational and memory load.  While it uses sparse matrices and matrix factorization in its computations, it is also to easily generate a vector of predictions in the same tidy format as the edx and validation dataframes.

The _recosystem_ package does not have the same variety of recommender algorithms and built-in functions for the examination of sparse rating matrix data as _recommenderlab_.  The package is a user-friendly R wrapper of a LIBMF library of various high-performance C++ computational approaches to large scale matrix factorization.  It includes algorithms for binary, real value and other types of rating matrices.







*RESULTS*

```{r, echo=FALSE}
mu <- mean(edx$rating)

res <- edx %>% 
  group_by(userId) %>%
  mutate(bu = mean(rating-mu)) %>% #user effect is adjusted through normalization
  ungroup() %>%
  group_by(movieId) %>%
  mutate(bi = sum(rating-mu)/(n()+.5)) %>% #movie effect is adjusted through regularization
  ungroup() %>%
  mutate(res = rating-bu-bi)#residuals found through removing user and movie effects

train <- data_memory(res$userId, res$movieId, rating = res$rating) 
#sparse matrix of residuals are basis for training matrix factorization model

r = Reco() #creating the Recommender System Object of the recosys package

r$train(train_data = train, opts = list(costq_l1 = 0))
#LIMBF model is trained using residual sparse matrix

valid <- data_memory(validation$userId, validation$movieId) 
#validation set userId and movieId data inputed into recosys environment for 
#prediction.  Note that the actual ratings are not entered into object.
#The validation set actual ratings will only be used for validating model prediction.

r$predict(test_data = valid)
#prediction function for recosys based on validation data - it 
#produces a file in the working directory that contains predictions. This can take a while.

pred <-read.csv("predict.txt", header = FALSE)
#reading the prediction file into the local environment.  Predictions are in CSV
#format without a header.

RMSE(validation$rating, pred$V1)
#.8324285 RMSE```

a results section that presents the modeling results and discusses the model performance




```{r}

```
*Conclusion*
conclusion section that gives a brief summary of the report, its limitations and future work